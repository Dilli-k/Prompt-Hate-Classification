{"metadata":{"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9246786,"sourceType":"datasetVersion","datasetId":5593841},{"sourceId":9246789,"sourceType":"datasetVersion","datasetId":5593844},{"sourceId":9276146,"sourceType":"datasetVersion","datasetId":5614222}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### PromptHate : Multimodal Classification with Prompt Based RoBERTa","metadata":{}},{"cell_type":"markdown","source":"#### utils.py","metadata":{}},{"cell_type":"code","source":"import errno\nimport os\nimport torch.nn as nn\nimport torch\nimport os\nimport json\nimport pickle as pkl\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nimport random\nimport pandas as pd\nimport h5py\n\n\ndef load_pkl(path):\n    data=pkl.load(open(path,'rb'))\n    return data\n\ndef read_hdf5(path):\n    data=h5py.File(path,'rb')\n    return data\n\ndef read_csv(path):\n    data=pd.read_csv(path)\n    return data\n\ndef read_csv_sep(path):\n    data=pd.read_csv(path,sep='\\t')\n    return data\n\ndef dump_pkl(path,info):\n    pkl.dump(info,open(path,'wb'))\n\ndef read_json(path):\n    # utils.assert_exits(path)\n    data=json.load(open(path,'rb'))\n    '''in anet-qa returns a list'''\n    return data\n\ndef pd_pkl(path):\n    data=pd.read_pickle(path)\n    return data\n\ndef read_jsonl(path):\n    total_info=[]\n    with open(path,'rb')as f:\n        d=f.readlines()\n    for i,info in enumerate(d):\n        data=json.loads(info)\n        total_info.append(data)\n    return total_info\n\n\nclass KLDivergence(nn.Module):\n    def __init__(self):\n        super(KLDivergence, self).__init__()\n\n    def forward(self,prob,logits):\n        #length = (prob.sum(-1) > 0.001).sum()\n        #print (prob.shape,logits.shape)\n        bz,obj=prob.shape\n        length=bz\n        #prob=torch.softmax(prob,-1)\n        #pred_prob = torch.softmax(logits, -1)\n        pred_prob=logits\n        #print (prob.sum(),pred_prob.sum())\n        print ('external:',pred_prob[0],\n               torch.sort(pred_prob[0],\n                          dim=0,\n               descending=True)[1][0])\n        print ('visual:',prob[0],\n               torch.sort(prob[0],\n                          dim=0,\n               descending=True)[1][0])\n        loss = -prob * torch.log(pred_prob)\n        loss = torch.sum(loss, -1)\n        loss = torch.sum(loss) / length\n        return loss\n\ndef assert_exits(path):\n    assert os.path.exists(path), 'Does not exist : {}'.format(path)\n\ndef equal_info(a,b):\n    assert len(a)==len(b),'File info not equal!'\n\ndef same_question(a,b):\n    assert a==b,'Not the same question!'\n\nclass Logger(object):\n    def __init__(self,output_dir):\n        dirname=os.path.dirname(output_dir)\n        if not os.path.exists(dirname):\n            os.mkdir(dirname)\n        self.log_file=open(output_dir,'w')\n        self.infos={}\n\n    def append(self,key,val):\n        vals=self.infos.setdefault(key,[])\n        vals.append(val)\n\n    def log(self,extra_msg=''):\n        msgs=[extra_msg]\n        for key, vals in self.infos.iteritems():\n            msgs.append('%s %.6f' %(key,np.mean(vals)))\n        msg='\\n'.joint(msgs)\n        self.log_file.write(msg+'\\n')\n        self.log_file.flush()\n        self.infos={}\n        return msg\n\n    def write(self,msg):\n        self.log_file.write(msg+'\\n')\n        self.log_file.flush()\n        print(msg)","metadata":{"id":"z0B_x2Av5gGH","execution":{"iopub.status.busy":"2024-08-29T20:32:35.253282Z","iopub.execute_input":"2024-08-29T20:32:35.254046Z","iopub.status.idle":"2024-08-29T20:32:35.275213Z","shell.execute_reply.started":"2024-08-29T20:32:35.253998Z","shell.execute_reply":"2024-08-29T20:32:35.274321Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Multimodal_Data.py","metadata":{}},{"cell_type":"code","source":"class Multimodal_Data():\n    #mem, off, harm\n    def __init__(self,opt,tokenizer,dataset,mode='train',few_shot_index=0):\n        super(Multimodal_Data,self).__init__()\n        self.opt=opt\n        self.tokenizer = tokenizer\n        self.mode=mode\n        if self.opt.FEW_SHOT:\n            self.few_shot_index=str(few_shot_index)\n            self.num_shots=self.opt.NUM_SHOTS\n            print ('Few shot learning setting for Iteration:',self.few_shot_index)\n            print ('Number of shots:',self.num_shots)\n        self.image_folder=self.opt.IMAGE_FOLDER\n        self.num_ans=self.opt.NUM_LABELS\n        #maximum length for a single sentence\n        self.length=self.opt.LENGTH\n        #maximum length of the concatenation of sentences\n        self.total_length=self.opt.TOTAL_LENGTH\n        self.num_sample=self.opt.NUM_SAMPLE\n        self.add_ent=self.opt.ADD_ENT\n        self.add_dem=self.opt.ADD_DEM\n        print ('Adding exntity information?',self.add_ent)\n        print ('Adding demographic information?',self.add_dem)\n        self.fine_grind=self.opt.FINE_GRIND\n        print ('Using target information?',self.fine_grind)\n\n        if opt.FINE_GRIND:\n            #target information\n            if self.opt.DATASET=='mem':\n                self.label_mapping_word={0:'nobody',\n                                         1:'race',\n                                         2:'disability',\n                                         3:'nationality',\n                                         4:'sex',\n                                         5:'religion'}\n            elif self.opt.DATASET=='harm':\n                self.label_mapping_word={0:'nobody',\n                                         1:'society',\n                                         2:'individual',\n                                         3:'community',\n                                         4:'organization'}\n                self.attack_list={'society':0,\n                                  'individual':1,\n                                  'community':2,\n                                  'organization':3}\n                self.attack_file=load_pkl(os.path.join(self.opt.DATA,\n                                                       'domain_splits','harm_trgt.pkl'))\n            self.template=\"*<s>**sent_0*.*_It_was_targeting*label_**</s>*\"\n        else:\n            self.label_mapping_word={0:self.opt.POS_WORD,\n                                     1:self.opt.NEG_WORD}\n            self.template=\"*<s>**sent_0*.*_It_was*label_**</s>*\"\n\n        self.label_mapping_id={}\n        for label in self.label_mapping_word.keys():\n            mapping_word=self.label_mapping_word[label]\n            #add space already\n            assert len(tokenizer.tokenize(' ' + self.label_mapping_word[label])) == 1\n            self.label_mapping_id[label] = \\\n            tokenizer._convert_token_to_id(\n                tokenizer.tokenize(' ' + self.label_mapping_word[label])[0])\n            print ('Mapping for label %d, word %s, index %d' %\n                   (label,mapping_word,self.label_mapping_id[label]))\n        #implementation for one template now\n\n\n        self.template_list=self.template.split('*')\n        print('Template:', self.template)\n        print('Template list:',self.template_list)\n        self.special_token_mapping = {\n            '<s>': tokenizer.convert_tokens_to_ids('<s>'),\n            '<mask>': tokenizer.mask_token_id,\n            '<pad>': tokenizer.pad_token_id, #1 for roberta\n            '</s>': tokenizer.convert_tokens_to_ids('<\\s>')\n        }\n\n        if self.opt.DEM_SAMP:\n            print ('Using demonstration sampling strategy...')\n            self.img_rate=self.opt.IMG_RATE\n            self.text_rate=self.opt.TEXT_RATE\n            self.samp_rate=self.opt.SIM_RATE\n            print ('Image rage for measuring CLIP similarity:',self.img_rate)\n            print ('Text rage for measuring CLIP similarity:',self.text_rate)\n            print ('Sampling from top:',self.samp_rate*100.0,'examples')\n            self.clip_clean=self.opt.CLIP_CLEAN\n            clip_path=os.path.join(\n                self.opt.CAPTION_PATH,\n                dataset,dataset+'_sim_scores.pkl')\n            print ('Clip feature path:',clip_path)\n            self.clip_feature=load_pkl(clip_path)\n\n        self.support_examples=self.load_entries('train')\n        print ('Length of supporting example:',len(self.support_examples))\n        self.entries=self.load_entries(mode)\n        if self.opt.DEBUG:\n            self.entries=self.entries[:128]\n        self.prepare_exp()\n        print ('The length of the dataset for:',mode,'is:',len(self.entries))\n\n    def load_entries(self,mode):\n        #print ('Loading data from:',self.dataset)\n        #only in training mode, in few-shot setting the loading will be different\n        if self.opt.FEW_SHOT and mode=='train':\n            path=os.path.join(self.opt.DATA,\n                              'domain-splits',\n                              self.opt.DATASET+'_'+str(self.num_shots)+'_'+self.few_shot_index+'.json')\n        else:\n            path=os.path.join(self.opt.DATA,\n                              'domain-splits',\n                              self.opt.DATASET+'_'+mode+'.json')\n        data=read_json(path)\n        cap_path=os.path.join(self.opt.CAPTION_PATH,\n                              self.opt.DATASET+'-'+self.opt.PRETRAIN_DATA,\n                              self.opt.IMG_VERSION+'_captions.pkl')\n        captions=load_pkl(cap_path)\n        entries=[]\n\n        for k,row in enumerate(data):\n            label=row['label']\n            img=row['img']\n            # img = os.path.join(self.image_folder, row['img'])\n            # print(img)\n            cap=captions[img.split('.')[0]][:-1]#remove the punctuation in the end\n            sent=row['clean_sent']\n            #remember the punctuations at the end of each sentence\n            cap=cap+' . '+sent+' . '\n            #whether using external knowledge\n            if self.add_ent:\n                cap=cap+' . '+row['entity']+' . '\n            if self.add_dem:\n                cap=cap+' . '+row['race']+' . '\n            entry={\n                'cap':cap.strip(),\n                'label':label,\n                'img':img\n            }\n            if self.fine_grind:\n                if self.opt.DATASET=='mem':\n                    if label==0:\n                        #[1,0,0,0,0,0]\n                        entry['attack']=[1]+row['attack']\n                    else:\n                        entry['attack']=[0]+row['attack']\n                elif self.opt.DATASET=='harm':\n                    if label==0:\n                        #[1,0,0,0,0,0]\n                        entry['attack']=[1,0,0,0,0]\n                    else:\n                        attack=[0,0,0,0,0]\n                        attack_idx=self.attack_list[self.attack_file[img]]+1\n                        attack[attack_idx]=1\n                        entry['attack']=attack\n            entries.append(entry)\n        return entries\n\n    def enc(self,text):\n        return self.tokenizer.encode(text, add_special_tokens=False)\n\n    def prepare_exp(self):\n        ###add sampling\n        support_indices = list(range(len(self.support_examples)))\n        self.example_idx = []\n        for sample_idx in tqdm(range(self.num_sample)):\n            for query_idx in range(len(self.entries)):\n                if self.opt.DEM_SAMP:\n                    #filter dissimilar demonstrations\n                    candidates= [support_idx for support_idx in support_indices\n                                 if support_idx != query_idx or self.mode != \"train\"]\n                    sim_score=[]\n                    count_each_label = {label: 0 for label in range(self.opt.NUM_LABELS)}\n                    context_indices=[]\n                    clip_info_que=self.clip_feature[self.entries[query_idx]['img']]\n\n                    #similarity computation\n                    for support_idx in candidates:\n                        img=self.support_examples[support_idx]['img']\n                        #this cost a lot of computation\n                        #unnormalized: the same scale -- 512 dimension\n                        if self.clip_clean:\n                            img_sim=clip_info_que['clean_img'][img]\n                        else:\n                            img_sim=clip_info_que['img'][img]\n                        text_sim=clip_info_que['text'][img]\n                        total_sim=self.img_rate*img_sim+self.text_rate*text_sim\n                        sim_score.append((support_idx,total_sim))\n                    sim_score.sort(key=lambda x: x[1],reverse=True)\n\n                    #top opt.SIM_RATE entities for each label\n                    num_valid=int(len(sim_score)//self.opt.NUM_LABELS*self.samp_rate)\n                    \"\"\"\n                    if self.opt.DEBUG:\n                        print ('Valid for each class:',num_valid)\n                    \"\"\"\n\n                    for support_idx, score in sim_score:\n                        cur_label=self.support_examples[support_idx]['label']\n                        if count_each_label[cur_label]<num_valid:\n                            count_each_label[cur_label]+=1\n                            context_indices.append(support_idx)\n                else:\n                    #exclude the current example during training\n                    context_indices = [support_idx for support_idx in support_indices\n                                       if support_idx != query_idx or self.mode != \"train\"]\n                #available indexes for supporting examples\n                self.example_idx.append((query_idx, context_indices, sample_idx))\n\n    def select_context(self, context_examples):\n        \"\"\"\n        Select demonstrations from provided examples.\n        \"\"\"\n        num_labels=self.opt.NUM_LABELS\n        max_demo_per_label = 1\n        counts = {k: 0 for k in range(num_labels)}\n        if num_labels == 1:\n            # Regression\n            counts = {'0': 0, '1': 0}\n        selection = []\n        \"\"\"\n        # Sampling strategy from LM-BFF\n        if self.opt.DEBUG:\n            print ('Number of context examples available:',len(context_examples))\n        \"\"\"\n        order = np.random.permutation(len(context_examples))\n        for i in order:\n            label = context_examples[i]['label']\n            if num_labels == 1:\n                # Regression\n                #No implementation currently\n                label = '0' if\\\n                float(label) <= median_mapping[self.args.task_name] else '1'\n            if counts[label] < max_demo_per_label:\n                selection.append(context_examples[i])\n                counts[label] += 1\n            if sum(counts.values()) == len(counts) * max_demo_per_label:\n                break\n\n        assert len(selection) > 0\n        return selection\n\n    def process_prompt(self, examples,\n                       first_sent_limit, other_sent_limit):\n        if self.fine_grind:\n            prompt_arch=' It was targeting '\n        else:\n            prompt_arch=' It was '\n        #currently, first and other limit are the same\n        input_ids = []\n        attention_mask = []\n        mask_pos = None # Position of the mask token\n        concat_sent=\"\"\n        for segment_id, ent in enumerate(examples):\n            #tokens for each example\n            new_tokens=[]\n            if segment_id==0:\n                #implementation for the querying example\n                new_tokens.append(self.special_token_mapping['<s>'])\n                length=first_sent_limit\n                temp=prompt_arch+'<mask>'+' . </s>'\n            else:\n                length=other_sent_limit\n                if self.fine_grind:\n                    if ent['label']==0:\n                        label_word=self.label_mapping_word[0]\n                    else:\n                        attack_types=[i for i, x in enumerate(ent['attack']) if x==1]\n                        #only for meme\n                        if len(attack_types)==0:\n                            attack_idx=random.randint(1,5)\n                        #randomly pick one\n                        #already padding nobody to the head of the list\n                        else:\n                            order=np.random.permutation(len(attack_types))\n                            attack_idx=attack_types[order[0]]\n                        label_word=self.label_mapping_word[attack_idx]\n                else:\n                    label_word=self.label_mapping_word[ent['label']]\n                temp=prompt_arch+label_word+' . </s>'\n            new_tokens+=self.enc(' '+ent['cap'])\n            #truncate the sentence if too long\n            new_tokens=new_tokens[:length]\n            new_tokens+=self.enc(temp)\n            whole_sent=' '+ent['cap']+temp\n            concat_sent+=whole_sent\n\n            #update the prompts\n            input_ids+=new_tokens\n            attention_mask += [1 for i in range(len(new_tokens))]\n        \"\"\"\n        if self.opt.DEBUG and self.opt.DEM_SAMP==False:\n            print (concat_sent)\n        \"\"\"\n        while len(input_ids) < self.total_length:\n            input_ids.append(self.special_token_mapping['<pad>'])\n            attention_mask.append(0)\n        if len(input_ids) > self.total_length:\n            input_ids = input_ids[:self.total_length]\n            attention_mask = attention_mask[:self.total_length]\n        mask_pos = [input_ids.index(self.special_token_mapping['<mask>'])]\n\n        # Make sure that the masked position is inside the max_length\n        assert mask_pos[0] < self.total_length\n        result = {'input_ids': input_ids,\n                  'sent':'<s>'+concat_sent,\n                  'attention_mask': attention_mask,\n                  'mask_pos': mask_pos}\n        return result\n\n\n    def __getitem__(self,index):\n        #query item\n        entry=self.entries[index]\n        #bootstrap_idx --> sample_idx\n        query_idx, context_indices, bootstrap_idx = self.example_idx[index]\n        #one example from each class\n        supports = self.select_context(\n            [self.support_examples[i] for i in context_indices])\n        exps=[]\n        exps.append(entry)\n        exps.extend(supports)\n        prompt_features = self.process_prompt(\n            exps,\n            self.length,\n            self.length\n        )\n\n        vid=entry['img']\n        #label=torch.tensor(self.label_mapping_id[entry['label']])\n        label=torch.tensor(entry['label'])\n        target=torch.from_numpy(np.zeros((self.num_ans),dtype=np.float32))\n        target[label]=1.0\n\n        cap_tokens=torch.Tensor(prompt_features['input_ids'])\n        mask_pos=torch.LongTensor(prompt_features['mask_pos'])\n        mask=torch.Tensor(prompt_features['attention_mask'])\n        batch={\n            'sent':prompt_features['sent'],\n            'mask':mask,\n            'img':vid,\n            'target':target,\n            'cap_tokens':cap_tokens,\n            'mask_pos':mask_pos,\n            'label':label\n        }\n        if self.fine_grind:\n            batch['attack']=torch.Tensor(entry['attack'])\n        #print (batch)\n        return batch\n\n    def __len__(self):\n        return len(self.entries)\n\n","metadata":{"id":"ViN1XG0R4768","execution":{"iopub.status.busy":"2024-08-29T20:32:35.583119Z","iopub.execute_input":"2024-08-29T20:32:35.583691Z","iopub.status.idle":"2024-08-29T20:32:35.642230Z","shell.execute_reply.started":"2024-08-29T20:32:35.583649Z","shell.execute_reply":"2024-08-29T20:32:35.641213Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import roc_auc_score\nfrom transformers import get_linear_schedule_with_warmup,AdamW\n\ndef bce_for_loss(logits,labels):\n    loss=nn.functional.binary_cross_entropy_with_logits(logits, labels)\n    loss*=labels.size(1)\n    return loss\n\ndef compute_auc_score(logits,label):\n    bz=logits.shape[0]\n    logits=logits.cpu().numpy()\n    label=label.cpu().numpy()\n    auc=roc_auc_score(label,logits,average='weighted')*bz\n    return auc\n\ndef compute_score(logits,labels):\n    #print (logits,logits.shape)\n    logits=torch.max(logits,1)[1]\n    #print (logits)\n    one_hot=torch.zeros(*labels.size()).cuda()\n    one_hot.scatter_(1,logits.view(-1,1),1)\n    score=one_hot * labels\n    return score.sum().float()\n\ndef compute_scaler_score(logits,labels):\n    #print (logits,logits.shape)\n    logits=torch.max(logits,1)[1]\n    labels=labels.squeeze(-1)\n    score=(logits==labels).int()\n    #print (score.sum(),labels,logits)\n    return score.sum().float()\n\n\ndef log_hyperpara(logger,opt):\n    dic = vars(opt)\n    for k,v in dic.items():\n        logger.write(k + ' : ' + str(v))\n\ndef train_for_epoch(opt,model,train_loader,test_loader):\n    #initialization of saving path\n    if opt.SAVE:\n        model_path=os.path.join('../models',\n                          '_'.join([opt.MODEL,opt.DATASET]))\n        if os.path.exists(model_path)==False:\n            os.mkdir(model_path)\n    #multi-qeury configuration\n    if opt.MULTI_QUERY:\n        from transformers import RobertaTokenizer\n#         tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n        tokenizer = RobertaTokenizer.from_pretrained(opt.BASE_MODEL)\n    #initialization of logger\n    log_path=os.path.join(opt.DATASET)\n    if os.path.exists(log_path)==False:\n        os.mkdir(log_path)\n    # logger=utils.Logger(os.path.join(log_path,str(opt.SAVE_NUM)+'.txt'))\n    # log_hyperpara(logger,opt)\n    # logger.write('Length of training set: %d, length of testing set: %d' %\n    #              (len(train_loader.dataset),len(test_loader.dataset)))\n\n    if opt.MODEL=='pbm':\n        #initialization of optimizer\n        params = {}\n        for n, p in model.named_parameters():\n            if opt.FIX_LAYERS > 0:\n                if 'encoder.layer' in n:\n                    try:\n                        layer_num = int(n[n.find('encoder.layer') + 14:].split('.')[0])\n                    except:\n                        print(n)\n                        raise Exception(\"\")\n                    if layer_num >= opt.FIX_LAYERS:\n                        print('yes', n)\n                        params[n] = p\n                    else:\n                        print('no ', n)\n                elif 'embeddings' in n:\n                    print('no ', n)\n                else:\n                    print('yes', n)\n                    params[n] = p\n            else:\n                params[n] = p\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in params.items() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": opt.WEIGHT_DECAY,\n            },\n            {\n                \"params\": [p for n, p in params.items() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n        optim = AdamW(\n            optimizer_grouped_parameters,\n            lr=opt.LR_RATE,\n            eps=opt.EPS,\n        )\n    else:\n        params=model.parameters()\n        optim=AdamW(params,\n                    lr=1e-5,\n                    eps=1e-8\n                   )\n    num_training_steps=len(train_loader) * opt.EPOCHS\n    scheduler=\\\n    get_linear_schedule_with_warmup(optim,\n                                    num_warmup_steps=0,\n                                    num_training_steps=num_training_steps\n                                   )\n    loss_fn=torch.nn.BCELoss()\n    loss_fct = nn.KLDivLoss(log_target=True)\n    #strat training\n    record_auc=[]\n    record_acc=[]\n    for epoch in range(opt.EPOCHS):\n        model.train(True)\n        total_loss=0.0\n        scores=0.0\n        for i,batch in enumerate(train_loader):\n            #break\n            cap=batch['cap_tokens'].long().cuda()\n            label=batch['label'].float().cuda().view(-1,1)\n            mask=batch['mask'].cuda()\n            target=batch['target'].cuda()\n            feat=None\n            if opt.MODEL=='pbm':\n                mask_pos=batch['mask_pos'].cuda()\n                logits=model(cap,mask,mask_pos,feat)\n                if opt.FINE_GRIND:\n                    attack=batch['attack'].cuda()#B,6\n                    #print (logits)\n                    #print (attack)\n                    #logits=logits*attack\n                    logits[:,1]=torch.sum(logits[:,1:],dim=1)\n                    logits=logits[:,:2]\n                #print (logits.shape,attack.shape)\n            elif opt.MODEL=='roberta':\n                if opt.UNIMODAL==False:\n                    feat=batch['feat'].cuda()\n                logits=model(cap,mask,feat)\n            loss=bce_for_loss(logits,target)\n            #print (logits,target)\n            batch_score=compute_score(logits,target)\n            scores+=batch_score\n            if i%100==0:\n              print ('Epoch:',epoch,'Iteration:', i, loss.item(),batch_score)\n            loss.backward()\n            optim.step()\n            scheduler.step()\n            optim.zero_grad()\n\n            total_loss+=loss\n\n\n        model.train(False)\n        scores/=len(train_loader.dataset)\n        if opt.MULTI_QUERY==False:\n            eval_acc,eval_auc=eval_model(opt,model,test_loader)\n        else:\n            eval_acc,eval_auc=eval_multi_model(opt,model,tokenizer)\n        record_auc.append(eval_auc)\n        record_acc.append(eval_acc)\n#         logger.write('Epoch %d' %(epoch))\n#         logger.write('\\ttrain_loss: %.2f, accuracy: %.2f' % (total_loss,\n#                                                              scores*100.0))\n#         logger.write('\\tevaluation auc: %.2f, accuracy: %.2f' % (eval_auc,\n#                                                                  eval_acc))\n    max_idx=sorted(range(len(record_auc)),\n                   key=lambda k: record_auc[k]+record_acc[k],\n                   reverse=True)[0]\n#     logger.write('Maximum epoch: %d' %(max_idx))\n#     logger.write('\\tevaluation auc: %.2f, accuracy: %.2f' % (record_auc[max_idx],\n#                                                              record_acc[max_idx]))\n\ndef eval_model(opt,model,test_loader):\n    scores=0.0\n    auc=0.0\n    len_data=len(test_loader.dataset)\n    print ('Length of test set:',len_data)\n    total_logits=[]\n    total_labels=[]\n    for i,batch in enumerate(test_loader):\n        with torch.no_grad():\n            cap=batch['cap_tokens'].long().cuda()\n            label=batch['label'].float().cuda().view(-1,1)\n            mask=batch['mask'].cuda()\n            target=batch['target'].cuda()\n            feat=None\n            if opt.MODEL=='pbm':\n                mask_pos=batch['mask_pos'].cuda()\n                logits=model(cap,mask,mask_pos,feat)\n                if opt.FINE_GRIND:\n                    #attack=batch['attack'].cuda()#B,6\n                    #logits=logits*attack\n                    logits[:,1]=torch.sum(logits[:,1:],dim=1)\n                    logits=logits[:,:2]\n\n            elif opt.MODEL=='roberta':\n                if opt.UNIMODAL==False:\n                    feat=batch['feat'].cuda()\n                logits=model(cap,mask,feat)\n\n            batch_score=compute_score(logits,target)\n            scores+=batch_score\n            norm_logits=F.softmax(logits,dim=-1)[:,1].unsqueeze(-1)\n\n            total_logits.append(norm_logits)\n            total_labels.append(label)\n    total_logits=torch.cat(total_logits,dim=0)\n    total_labels=torch.cat(total_labels,dim=0)\n    print (total_logits.shape,total_labels.shape)\n    auc=compute_auc_score(total_logits,total_labels)\n    #print (auc)\n    return scores*100.0/len_data,auc*100.0/len_data\n\ndef eval_multi_model(opt,model, tokenizer):\n    num_queries=opt.NUM_QUERIES\n    labels_record={}\n    logits_record={}\n    prob_record={}\n    for k in range(num_queries):\n        test_set=Multimodal_Data(opt,tokenizer,opt.DATASET,'test')\n        test_loader=DataLoader(test_set,\n                               opt.BATCH_SIZE,\n                               shuffle=False,\n                               num_workers=1)\n        len_data=len(test_loader.dataset)\n        print ('Length of test set:',len_data,'Query:',k)\n        for i,batch in enumerate(test_loader):\n            with torch.no_grad():\n                cap=batch['cap_tokens'].long().cuda()\n                label=batch['label'].float().cuda().view(-1,1)\n                mask=batch['mask'].cuda()\n                mask_pos=batch['mask_pos'].cuda()\n                logits=model(cap,mask,mask_pos)\n                if opt.FINE_GRIND:\n                    #attack=batch['attack'].cuda()#B,6\n                    #logits=logits*attack\n                    logits[:,1]=torch.sum(logits[:,1:],dim=1)\n                    logits=logits[:,:2]\n                target=batch['target'].cuda()\n                img=batch['img']\n                norm_prob=F.softmax(logits,dim=-1)\n                norm_logits=norm_prob[:,1].unsqueeze(-1)\n\n                bz=cap.shape[0]\n                for j in range(bz):\n                    cur_img=img[j]\n                    cur_logits=norm_logits[j:j+1]\n                    #should normalize to the same scale\n                    cur_prob=norm_prob[j:j+1]\n                    if k==0:\n                        cur_label=label[j:j+1]\n                        labels_record[cur_img]=cur_label\n                        logits_record[cur_img]=cur_logits\n                        prob_record[cur_img]=cur_prob\n                    else:\n                        logits_record[cur_img]+=cur_logits\n                        prob_record[cur_img]+=cur_prob\n    labels=[]\n    logits=[]\n    probs=[]\n    for name in labels_record.keys():\n        labels.append(labels_record[name])\n        logits.append(logits_record[name]/num_queries)\n        probs.append(prob_record[name]/num_queries)\n\n    logits=torch.cat(logits,dim=0)\n    labels=torch.cat(labels,dim=0)\n    probs=torch.cat(probs,dim=0)\n\n    scores=compute_scaler_score(probs,labels)\n    auc=compute_auc_score(logits,labels)\n    print(auc)\n    return scores*100.0/len_data,auc*100.0/len_data\n","metadata":{"id":"Fy6jtgdb476-","execution":{"iopub.status.busy":"2024-08-29T20:32:35.644335Z","iopub.execute_input":"2024-08-29T20:32:35.644657Z","iopub.status.idle":"2024-08-29T20:32:35.694353Z","shell.execute_reply.started":"2024-08-29T20:32:35.644624Z","shell.execute_reply":"2024-08-29T20:32:35.693382Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass ModelConfig:\n    DATASET: str\n    FEW_SHOT: bool\n    FINE_GRIND: bool\n    NUM_SHOTS: int\n    MODEL: str\n    UNIMODAL: bool\n    DATA: str\n    CAPTION_PATH: str\n    RESULT: str\n    FEAT_DIM: int\n    CLIP_DIM: int\n    BERT_DIM: int\n    ROBERTA_DIM: int\n    NUM_FOLD: int\n    EMB_DIM: int\n    NUM_LABELS: int\n    POS_WORD: str\n    NEG_WORD: str\n    DEM_SAMP: bool\n    SIM_RATE: float\n    IMG_RATE: float\n    TEXT_RATE: float\n    CLIP_CLEAN: bool\n    MULTI_QUERY: bool\n    NUM_QUERIES: int\n    EMB_DROPOUT: float\n    FC_DROPOUT: float\n    WEIGHT_DECAY: float\n    LR_RATE: float\n    EPS: float\n    BATCH_SIZE: int\n    FIX_LAYERS: int\n    MID_DIM: int\n    NUM_HIDDEN: int\n    LENGTH: int\n    TOTAL_LENGTH: int\n    PREFIX_LENGTH: int\n    NUM_SAMPLE: int\n    NUM_LAYER: int\n    MODEL_NAME: str\n    PRETRAIN_DATA: str\n    IMG_VERSION: str\n    MAPPING_TYPE: str\n    ADD_ENT: bool\n    ADD_DEM: bool\n    DEBUG: bool\n    SAVE: bool\n    SAVE_NUM: int\n    EPOCHS: int\n    SEED: int\n    CUDA_DEVICE: int\n    WARM_UP: int\n    TRANS_LAYER: int\n    NUM_HEAD: int\n    IMAGE_FOLDER:str\n    BASE_MODEL:str\n\n\n","metadata":{"id":"J8Hixhza476_","execution":{"iopub.status.busy":"2024-08-29T20:32:35.695646Z","iopub.execute_input":"2024-08-29T20:32:35.695981Z","iopub.status.idle":"2024-08-29T20:32:35.715680Z","shell.execute_reply.started":"2024-08-29T20:32:35.695948Z","shell.execute_reply":"2024-08-29T20:32:35.714743Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import RobertaForMaskedLM\nfrom transformers import DistilBertForMaskedLM\nclass RobertaPromptModel(nn.Module):\n    def __init__(self,label_list,base_model):\n        super(RobertaPromptModel, self).__init__()\n        self.label_word_list=label_list\n        self.roberta = RobertaForMaskedLM.from_pretrained(base_model)\n#         self.roberta = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n    def forward(self,tokens,attention_mask,mask_pos,feat=None):\n        batch_size = tokens.size(0)\n        #the position of word for prediction\n        if mask_pos is not None:\n            mask_pos = mask_pos.squeeze()\n\n        out = self.roberta(tokens,\n                           attention_mask)\n        prediction_mask_scores = out.logits[torch.arange(batch_size),\n                                          mask_pos]\n\n        logits = []\n        for label_id in range(len(self.label_word_list)):\n            logits.append(prediction_mask_scores[:,\n                                                 self.label_word_list[label_id]\n                                                ].unsqueeze(-1))\n            #print(prediction_mask_scores[:, self.label_word_list[label_id]].shape)\n        logits = torch.cat(logits, -1)\n        #print(logits.shape)\n        return logits\n\n\ndef build_baseline(opt,label_list):\n    print (label_list)\n    return RobertaPromptModel(label_list,opt.BASE_MODEL)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:32:35.716779Z","iopub.execute_input":"2024-08-29T20:32:35.717074Z","iopub.status.idle":"2024-08-29T20:32:35.734237Z","shell.execute_reply.started":"2024-08-29T20:32:35.717041Z","shell.execute_reply":"2024-08-29T20:32:35.733323Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\n\n# import config\nimport os\n# from train import train_for_epoch\nfrom torch.utils.data import DataLoader\nfrom transformers import RobertaTokenizer\nfrom transformers import DistilBertTokenizer\n\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\n    \n# Example of creating an instance\n# change the epochs here and run the model again \nconfig = ModelConfig(\n    DATASET=\"mem\", FEW_SHOT=False, FINE_GRIND=False, NUM_SHOTS=16, MODEL=\"pbm\", UNIMODAL=False,\n    DATA=\"/kaggle/input\", CAPTION_PATH=\"/kaggle/input\", RESULT=\"/kaggle/working\", FEAT_DIM=2048, CLIP_DIM=512,\n    BERT_DIM=768, ROBERTA_DIM=1024, NUM_FOLD=5, EMB_DIM=300, NUM_LABELS=2, POS_WORD=\"good\",\n    NEG_WORD=\"bad\", DEM_SAMP=False, SIM_RATE=0.5, IMG_RATE=0.5, TEXT_RATE=0.5, CLIP_CLEAN=False,\n    MULTI_QUERY=True, NUM_QUERIES=4, EMB_DROPOUT=0.0, FC_DROPOUT=0.4, WEIGHT_DECAY=0.01, LR_RATE=1.3e-5,\n    EPS=1e-8, BATCH_SIZE=16, FIX_LAYERS=2, MID_DIM=512, NUM_HIDDEN=512, LENGTH=64, TOTAL_LENGTH=256,\n    PREFIX_LENGTH=10, NUM_SAMPLE=1, NUM_LAYER=8, MODEL_NAME=\"roberta-large\", PRETRAIN_DATA=\"conceptual\",\n    IMG_VERSION=\"clean\", MAPPING_TYPE=\"transformer\", ADD_ENT=True, ADD_DEM=True, DEBUG=False, SAVE=False,\n    SAVE_NUM=100, EPOCHS=1, SEED=1111, CUDA_DEVICE=15, WARM_UP=2000, TRANS_LAYER=1, NUM_HEAD=8,IMAGE_FOLDER='/content/drive/MyDrive/meme/data',BASE_MODEL='roberta-base'\n)\nif __name__=='__main__':\n    opt=config\n    # torch.cuda.set_device(opt.CUDA_DEVICE)\n    set_seed(opt.SEED)\n\n\n    # Create tokenizer\n    tokenizer = RobertaTokenizer.from_pretrained(opt.BASE_MODEL)\n#     tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    constructor='build_baseline'\n    if opt.MODEL=='pbm':\n        # from dataset import Multimodal_Data\n        # import baseline\n        train_set=Multimodal_Data(opt,tokenizer,opt.DATASET,'train',opt.SEED-1111)\n        test_set=Multimodal_Data(opt,tokenizer,opt.DATASET,'test')\n        label_list=[train_set.label_mapping_id[i] for i in train_set.label_mapping_word.keys()]\n        model = build_baseline(opt, label_list).cuda()\n    else:\n        # from roberta_dataset import Roberta_Data\n        # import roberta_baseline\n        # train_set=Roberta_Data(opt,tokenizer,opt.DATASET,'train',opt.SEED-1111)\n        # test_set=Roberta_Data(opt,tokenizer,opt.DATASET,'test')\n        # model=getattr(roberta_baseline,constructor)(opt).cuda()\n        pass\n\n    train_loader=DataLoader(train_set,\n                            opt.BATCH_SIZE,\n                            shuffle=True,\n                            num_workers=1)\n    test_loader=DataLoader(test_set,\n                           opt.BATCH_SIZE,\n                           shuffle=False,\n                           num_workers=1)\n    train_for_epoch(opt,model,train_loader,test_loader)\n\n#     exit(0)\n","metadata":{"id":"mxtWEYj-476_","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e3d21e55-d375-4a44-f4ae-a55c5aca4c17","execution":{"iopub.status.busy":"2024-08-29T20:32:35.737031Z","iopub.execute_input":"2024-08-29T20:32:35.737602Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Adding exntity information? True\nAdding demographic information? True\nUsing target information? False\nMapping for label 0, word good, index 205\nMapping for label 1, word bad, index 1099\nTemplate: *<s>**sent_0*.*_It_was*label_**</s>*\nTemplate list: ['', '<s>', '', 'sent_0', '.', '_It_was', 'label_', '', '</s>', '']\nLength of supporting example: 8500\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:06<00:00,  6.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"The length of the dataset for: train is: 8500\nAdding exntity information? True\nAdding demographic information? True\nUsing target information? False\nMapping for label 0, word good, index 205\nMapping for label 1, word bad, index 1099\nTemplate: *<s>**sent_0*.*_It_was*label_**</s>*\nTemplate list: ['', '<s>', '', 'sent_0', '.', '_It_was', 'label_', '', '</s>', '']\nLength of supporting example: 8500\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  2.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"The length of the dataset for: test is: 500\n[205, 1099]\nno  roberta.roberta.embeddings.word_embeddings.weight\nno  roberta.roberta.embeddings.position_embeddings.weight\nno  roberta.roberta.embeddings.token_type_embeddings.weight\nno  roberta.roberta.embeddings.LayerNorm.weight\nno  roberta.roberta.embeddings.LayerNorm.bias\nno  roberta.roberta.encoder.layer.0.attention.self.query.weight\nno  roberta.roberta.encoder.layer.0.attention.self.query.bias\nno  roberta.roberta.encoder.layer.0.attention.self.key.weight\nno  roberta.roberta.encoder.layer.0.attention.self.key.bias\nno  roberta.roberta.encoder.layer.0.attention.self.value.weight\nno  roberta.roberta.encoder.layer.0.attention.self.value.bias\nno  roberta.roberta.encoder.layer.0.attention.output.dense.weight\nno  roberta.roberta.encoder.layer.0.attention.output.dense.bias\nno  roberta.roberta.encoder.layer.0.attention.output.LayerNorm.weight\nno  roberta.roberta.encoder.layer.0.attention.output.LayerNorm.bias\nno  roberta.roberta.encoder.layer.0.intermediate.dense.weight\nno  roberta.roberta.encoder.layer.0.intermediate.dense.bias\nno  roberta.roberta.encoder.layer.0.output.dense.weight\nno  roberta.roberta.encoder.layer.0.output.dense.bias\nno  roberta.roberta.encoder.layer.0.output.LayerNorm.weight\nno  roberta.roberta.encoder.layer.0.output.LayerNorm.bias\nno  roberta.roberta.encoder.layer.1.attention.self.query.weight\nno  roberta.roberta.encoder.layer.1.attention.self.query.bias\nno  roberta.roberta.encoder.layer.1.attention.self.key.weight\nno  roberta.roberta.encoder.layer.1.attention.self.key.bias\nno  roberta.roberta.encoder.layer.1.attention.self.value.weight\nno  roberta.roberta.encoder.layer.1.attention.self.value.bias\nno  roberta.roberta.encoder.layer.1.attention.output.dense.weight\nno  roberta.roberta.encoder.layer.1.attention.output.dense.bias\nno  roberta.roberta.encoder.layer.1.attention.output.LayerNorm.weight\nno  roberta.roberta.encoder.layer.1.attention.output.LayerNorm.bias\nno  roberta.roberta.encoder.layer.1.intermediate.dense.weight\nno  roberta.roberta.encoder.layer.1.intermediate.dense.bias\nno  roberta.roberta.encoder.layer.1.output.dense.weight\nno  roberta.roberta.encoder.layer.1.output.dense.bias\nno  roberta.roberta.encoder.layer.1.output.LayerNorm.weight\nno  roberta.roberta.encoder.layer.1.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.2.attention.self.query.weight\nyes roberta.roberta.encoder.layer.2.attention.self.query.bias\nyes roberta.roberta.encoder.layer.2.attention.self.key.weight\nyes roberta.roberta.encoder.layer.2.attention.self.key.bias\nyes roberta.roberta.encoder.layer.2.attention.self.value.weight\nyes roberta.roberta.encoder.layer.2.attention.self.value.bias\nyes roberta.roberta.encoder.layer.2.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.2.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.2.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.2.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.2.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.2.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.2.output.dense.weight\nyes roberta.roberta.encoder.layer.2.output.dense.bias\nyes roberta.roberta.encoder.layer.2.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.2.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.3.attention.self.query.weight\nyes roberta.roberta.encoder.layer.3.attention.self.query.bias\nyes roberta.roberta.encoder.layer.3.attention.self.key.weight\nyes roberta.roberta.encoder.layer.3.attention.self.key.bias\nyes roberta.roberta.encoder.layer.3.attention.self.value.weight\nyes roberta.roberta.encoder.layer.3.attention.self.value.bias\nyes roberta.roberta.encoder.layer.3.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.3.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.3.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.3.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.3.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.3.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.3.output.dense.weight\nyes roberta.roberta.encoder.layer.3.output.dense.bias\nyes roberta.roberta.encoder.layer.3.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.3.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.4.attention.self.query.weight\nyes roberta.roberta.encoder.layer.4.attention.self.query.bias\nyes roberta.roberta.encoder.layer.4.attention.self.key.weight\nyes roberta.roberta.encoder.layer.4.attention.self.key.bias\nyes roberta.roberta.encoder.layer.4.attention.self.value.weight\nyes roberta.roberta.encoder.layer.4.attention.self.value.bias\nyes roberta.roberta.encoder.layer.4.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.4.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.4.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.4.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.4.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.4.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.4.output.dense.weight\nyes roberta.roberta.encoder.layer.4.output.dense.bias\nyes roberta.roberta.encoder.layer.4.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.4.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.5.attention.self.query.weight\nyes roberta.roberta.encoder.layer.5.attention.self.query.bias\nyes roberta.roberta.encoder.layer.5.attention.self.key.weight\nyes roberta.roberta.encoder.layer.5.attention.self.key.bias\nyes roberta.roberta.encoder.layer.5.attention.self.value.weight\nyes roberta.roberta.encoder.layer.5.attention.self.value.bias\nyes roberta.roberta.encoder.layer.5.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.5.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.5.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.5.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.5.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.5.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.5.output.dense.weight\nyes roberta.roberta.encoder.layer.5.output.dense.bias\nyes roberta.roberta.encoder.layer.5.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.5.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.6.attention.self.query.weight\nyes roberta.roberta.encoder.layer.6.attention.self.query.bias\nyes roberta.roberta.encoder.layer.6.attention.self.key.weight\nyes roberta.roberta.encoder.layer.6.attention.self.key.bias\nyes roberta.roberta.encoder.layer.6.attention.self.value.weight\nyes roberta.roberta.encoder.layer.6.attention.self.value.bias\nyes roberta.roberta.encoder.layer.6.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.6.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.6.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.6.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.6.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.6.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.6.output.dense.weight\nyes roberta.roberta.encoder.layer.6.output.dense.bias\nyes roberta.roberta.encoder.layer.6.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.6.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.7.attention.self.query.weight\nyes roberta.roberta.encoder.layer.7.attention.self.query.bias\nyes roberta.roberta.encoder.layer.7.attention.self.key.weight\nyes roberta.roberta.encoder.layer.7.attention.self.key.bias\nyes roberta.roberta.encoder.layer.7.attention.self.value.weight\nyes roberta.roberta.encoder.layer.7.attention.self.value.bias\nyes roberta.roberta.encoder.layer.7.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.7.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.7.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.7.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.7.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.7.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.7.output.dense.weight\nyes roberta.roberta.encoder.layer.7.output.dense.bias\nyes roberta.roberta.encoder.layer.7.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.7.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.8.attention.self.query.weight\nyes roberta.roberta.encoder.layer.8.attention.self.query.bias\nyes roberta.roberta.encoder.layer.8.attention.self.key.weight\nyes roberta.roberta.encoder.layer.8.attention.self.key.bias\nyes roberta.roberta.encoder.layer.8.attention.self.value.weight\nyes roberta.roberta.encoder.layer.8.attention.self.value.bias\nyes roberta.roberta.encoder.layer.8.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.8.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.8.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.8.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.8.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.8.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.8.output.dense.weight\nyes roberta.roberta.encoder.layer.8.output.dense.bias\nyes roberta.roberta.encoder.layer.8.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.8.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.9.attention.self.query.weight\nyes roberta.roberta.encoder.layer.9.attention.self.query.bias\nyes roberta.roberta.encoder.layer.9.attention.self.key.weight\nyes roberta.roberta.encoder.layer.9.attention.self.key.bias\nyes roberta.roberta.encoder.layer.9.attention.self.value.weight\nyes roberta.roberta.encoder.layer.9.attention.self.value.bias\nyes roberta.roberta.encoder.layer.9.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.9.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.9.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.9.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.9.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.9.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.9.output.dense.weight\nyes roberta.roberta.encoder.layer.9.output.dense.bias\nyes roberta.roberta.encoder.layer.9.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.9.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.10.attention.self.query.weight\nyes roberta.roberta.encoder.layer.10.attention.self.query.bias\nyes roberta.roberta.encoder.layer.10.attention.self.key.weight\nyes roberta.roberta.encoder.layer.10.attention.self.key.bias\nyes roberta.roberta.encoder.layer.10.attention.self.value.weight\nyes roberta.roberta.encoder.layer.10.attention.self.value.bias\nyes roberta.roberta.encoder.layer.10.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.10.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.10.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.10.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.10.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.10.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.10.output.dense.weight\nyes roberta.roberta.encoder.layer.10.output.dense.bias\nyes roberta.roberta.encoder.layer.10.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.10.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.11.attention.self.query.weight\nyes roberta.roberta.encoder.layer.11.attention.self.query.bias\nyes roberta.roberta.encoder.layer.11.attention.self.key.weight\nyes roberta.roberta.encoder.layer.11.attention.self.key.bias\nyes roberta.roberta.encoder.layer.11.attention.self.value.weight\nyes roberta.roberta.encoder.layer.11.attention.self.value.bias\nyes roberta.roberta.encoder.layer.11.attention.output.dense.weight\nyes roberta.roberta.encoder.layer.11.attention.output.dense.bias\nyes roberta.roberta.encoder.layer.11.attention.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.11.attention.output.LayerNorm.bias\nyes roberta.roberta.encoder.layer.11.intermediate.dense.weight\nyes roberta.roberta.encoder.layer.11.intermediate.dense.bias\nyes roberta.roberta.encoder.layer.11.output.dense.weight\nyes roberta.roberta.encoder.layer.11.output.dense.bias\nyes roberta.roberta.encoder.layer.11.output.LayerNorm.weight\nyes roberta.roberta.encoder.layer.11.output.LayerNorm.bias\nyes roberta.lm_head.bias\nyes roberta.lm_head.dense.weight\nyes roberta.lm_head.dense.bias\nyes roberta.lm_head.layer_norm.weight\nyes roberta.lm_head.layer_norm.bias\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0 Iteration: 0 13.89920425415039 tensor(11., device='cuda:0')\nEpoch: 0 Iteration: 100 1.3086106777191162 tensor(10., device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"    # Save the model\n    model_save_path = os.path.join(opt.RESULT, 'model.pth')\n    torch.save(model.state_dict(), model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n","metadata":{"id":"VRpsh6R-9u1s","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Only run this code if you run changing epochs\n","metadata":{}},{"cell_type":"code","source":"opt = ModelConfig(\n    DATASET=\"mem\", FEW_SHOT=False, FINE_GRIND=False, NUM_SHOTS=16, MODEL=\"pbm\", UNIMODAL=False,\n    DATA=\"/kaggle/input\", CAPTION_PATH=\"/kaggle/input\", RESULT=\"/kaggle/working\", FEAT_DIM=2048, CLIP_DIM=512,\n    BERT_DIM=768, ROBERTA_DIM=1024, NUM_FOLD=5, EMB_DIM=300, NUM_LABELS=2, POS_WORD=\"good\",\n    NEG_WORD=\"bad\", DEM_SAMP=False, SIM_RATE=0.5, IMG_RATE=0.5, TEXT_RATE=0.5, CLIP_CLEAN=False,\n    MULTI_QUERY=True, NUM_QUERIES=4, EMB_DROPOUT=0.0, FC_DROPOUT=0.4, WEIGHT_DECAY=0.01, LR_RATE=1.3e-5,\n    EPS=1e-8, BATCH_SIZE=16, FIX_LAYERS=2, MID_DIM=512, NUM_HIDDEN=512, LENGTH=64, TOTAL_LENGTH=256,\n    PREFIX_LENGTH=10, NUM_SAMPLE=1, NUM_LAYER=8, MODEL_NAME=\"roberta-large\", PRETRAIN_DATA=\"conceptual\",\n    IMG_VERSION=\"clean\", MAPPING_TYPE=\"transformer\", ADD_ENT=True, ADD_DEM=True, DEBUG=False, SAVE=False,\n    SAVE_NUM=100, EPOCHS=1, SEED=1111, CUDA_DEVICE=15, WARM_UP=2000, TRANS_LAYER=1, NUM_HEAD=8,IMAGE_FOLDER='/content/drive/MyDrive/meme/data',\n    BASE_MODEL='roberta-base'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example label list (replace this with your actual label list)\ntokenizer = RobertaTokenizer.from_pretrained(opt.BASE_MODEL)\ntrain_set=Multimodal_Data(opt,tokenizer,opt.DATASET,'train',opt.SEED-1111)\nlabel_list = [train_set.label_mapping_id[i] for i in train_set.label_mapping_word.keys()]\n\n# Build the model\nmodel = build_baseline(opt, label_list)\ncompile\n# Load the state dictionary\nmodel_path = 'add the model path here '\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming `opt` is already defined and `eval_multi_model` is your evaluation function\nprint(eval_multi_model(opt, model, tokenizer))","metadata":{},"execution_count":null,"outputs":[]}]}
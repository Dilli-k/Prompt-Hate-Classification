{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9246786,"sourceType":"datasetVersion","datasetId":5593841},{"sourceId":9246789,"sourceType":"datasetVersion","datasetId":5593844},{"sourceId":9276146,"sourceType":"datasetVersion","datasetId":5614222}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport errno\nimport os\nimport torch.nn as nn\nimport torch\nimport os\nimport json\nimport pickle as pkl\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nimport random\nimport pandas as pd\nimport h5py\nimport os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import roc_auc_score\nfrom transformers import get_linear_schedule_with_warmup,AdamW\n\ndef load_pkl(path):\n    data=pkl.load(open(path,'rb'))\n    return data\n\ndef read_json(path):\n    # utils.assert_exits(path)\n    data=json.load(open(path,'rb'))\n    '''in anet-qa returns a list'''\n    return data\ndef bce_for_loss(logits,labels):\n    loss=nn.functional.binary_cross_entropy_with_logits(logits, labels)\n    loss*=labels.size(1)\n    return loss\n\ndef compute_auc_score(logits,label):\n    bz=logits.shape[0]\n    logits=logits.cpu().numpy()\n    label=label.cpu().numpy()\n    auc=roc_auc_score(label,logits,average='weighted')*bz\n    return auc\n\ndef compute_score(logits,labels):\n    #print (logits,logits.shape)\n    logits=torch.max(logits,1)[1]\n    #print (logits)\n    one_hot=torch.zeros(*labels.size()).cuda()\n    one_hot.scatter_(1,logits.view(-1,1),1)\n    score=one_hot * labels\n    return score.sum().float()\n\ndef compute_scaler_score(logits,labels):\n    #print (logits,logits.shape)\n    logits=torch.max(logits,1)[1]\n    labels=labels.squeeze(-1)\n    score=(logits==labels).int()\n    #print (score.sum(),labels,logits)\n    return score.sum().float()\n\n\ndef log_hyperpara(logger,opt):\n    dic = vars(opt)\n    for k,v in dic.items():\n        logger.write(k + ' : ' + str(v))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:18:51.776939Z","iopub.execute_input":"2024-08-29T20:18:51.777404Z","iopub.status.idle":"2024-08-29T20:18:51.792956Z","shell.execute_reply.started":"2024-08-29T20:18:51.777339Z","shell.execute_reply":"2024-08-29T20:18:51.791756Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class Multimodal_Data():\n    #mem, off, harm\n    def __init__(self,opt,tokenizer,dataset,mode='train',few_shot_index=0):\n        super(Multimodal_Data,self).__init__()\n        self.opt=opt\n        self.tokenizer = tokenizer\n        self.mode=mode\n        if self.opt.FEW_SHOT:\n            self.few_shot_index=str(few_shot_index)\n            self.num_shots=self.opt.NUM_SHOTS\n            print ('Few shot learning setting for Iteration:',self.few_shot_index)\n            print ('Number of shots:',self.num_shots)\n        self.image_folder=self.opt.IMAGE_FOLDER\n        self.num_ans=self.opt.NUM_LABELS\n        #maximum length for a single sentence\n        self.length=self.opt.LENGTH\n        #maximum length of the concatenation of sentences\n        self.total_length=self.opt.TOTAL_LENGTH\n        self.num_sample=self.opt.NUM_SAMPLE\n        self.add_ent=self.opt.ADD_ENT\n        self.add_dem=self.opt.ADD_DEM\n        print ('Adding exntity information?',self.add_ent)\n        print ('Adding demographic information?',self.add_dem)\n        self.fine_grind=self.opt.FINE_GRIND\n        print ('Using target information?',self.fine_grind)\n\n        if opt.FINE_GRIND:\n            #target information\n            if self.opt.DATASET=='mem':\n                self.label_mapping_word={0:'nobody',\n                                         1:'race',\n                                         2:'disability',\n                                         3:'nationality',\n                                         4:'sex',\n                                         5:'religion'}\n            elif self.opt.DATASET=='harm':\n                self.label_mapping_word={0:'nobody',\n                                         1:'society',\n                                         2:'individual',\n                                         3:'community',\n                                         4:'organization'}\n                self.attack_list={'society':0,\n                                  'individual':1,\n                                  'community':2,\n                                  'organization':3}\n                self.attack_file=load_pkl(os.path.join(self.opt.DATA,\n                                                       'domain_splits','harm_trgt.pkl'))\n            self.template=\"*<s>**sent_0*.*_It_was_targeting*label_**</s>*\"\n        else:\n            self.label_mapping_word={0:self.opt.POS_WORD,\n                                     1:self.opt.NEG_WORD}\n            self.template=\"*<s>**sent_0*.*_It_was*label_**</s>*\"\n\n        self.label_mapping_id={}\n        for label in self.label_mapping_word.keys():\n            mapping_word=self.label_mapping_word[label]\n            #add space already\n            assert len(tokenizer.tokenize(' ' + self.label_mapping_word[label])) == 1\n            self.label_mapping_id[label] = \\\n            tokenizer._convert_token_to_id(\n                tokenizer.tokenize(' ' + self.label_mapping_word[label])[0])\n            print ('Mapping for label %d, word %s, index %d' %\n                   (label,mapping_word,self.label_mapping_id[label]))\n        #implementation for one template now\n\n\n        self.template_list=self.template.split('*')\n        print('Template:', self.template)\n        print('Template list:',self.template_list)\n        self.special_token_mapping = {\n            '<s>': tokenizer.convert_tokens_to_ids('<s>'),\n            '<mask>': tokenizer.mask_token_id,\n            '<pad>': tokenizer.pad_token_id, #1 for roberta\n            '</s>': tokenizer.convert_tokens_to_ids('<\\s>')\n        }\n\n        if self.opt.DEM_SAMP:\n            print ('Using demonstration sampling strategy...')\n            self.img_rate=self.opt.IMG_RATE\n            self.text_rate=self.opt.TEXT_RATE\n            self.samp_rate=self.opt.SIM_RATE\n            print ('Image rage for measuring CLIP similarity:',self.img_rate)\n            print ('Text rage for measuring CLIP similarity:',self.text_rate)\n            print ('Sampling from top:',self.samp_rate*100.0,'examples')\n            self.clip_clean=self.opt.CLIP_CLEAN\n            clip_path=os.path.join(\n                self.opt.CAPTION_PATH,\n                dataset,dataset+'_sim_scores.pkl')\n            print ('Clip feature path:',clip_path)\n            self.clip_feature=load_pkl(clip_path)\n\n        self.support_examples=self.load_entries('train')\n        print ('Length of supporting example:',len(self.support_examples))\n        self.entries=self.load_entries(mode)\n        if self.opt.DEBUG:\n            self.entries=self.entries[:128]\n        self.prepare_exp()\n        print ('The length of the dataset for:',mode,'is:',len(self.entries))\n\n    def load_entries(self,mode):\n        #print ('Loading data from:',self.dataset)\n        #only in training mode, in few-shot setting the loading will be different\n        if self.opt.FEW_SHOT and mode=='train':\n            path=os.path.join(self.opt.DATA,\n                              'domain-splits',\n                              self.opt.DATASET+'_'+str(self.num_shots)+'_'+self.few_shot_index+'.json')\n        else:\n            path=os.path.join(self.opt.DATA,\n                              'domain-splits',\n                              self.opt.DATASET+'_'+mode+'.json')\n        data=read_json(path)\n        cap_path=os.path.join(self.opt.CAPTION_PATH,\n                              self.opt.DATASET+'-'+self.opt.PRETRAIN_DATA,\n                              self.opt.IMG_VERSION+'_captions.pkl')\n        captions=load_pkl(cap_path)\n        entries=[]\n\n        for k,row in enumerate(data):\n            label=row['label']\n            img=row['img']\n            # img = os.path.join(self.image_folder, row['img'])\n            # print(img)\n            cap=captions[img.split('.')[0]][:-1]#remove the punctuation in the end\n            sent=row['clean_sent']\n            #remember the punctuations at the end of each sentence\n            cap=cap+' . '+sent+' . '\n            #whether using external knowledge\n            if self.add_ent:\n                cap=cap+' . '+row['entity']+' . '\n            if self.add_dem:\n                cap=cap+' . '+row['race']+' . '\n            entry={\n                'cap':cap.strip(),\n                'label':label,\n                'img':img\n            }\n            if self.fine_grind:\n                if self.opt.DATASET=='mem':\n                    if label==0:\n                        #[1,0,0,0,0,0]\n                        entry['attack']=[1]+row['attack']\n                    else:\n                        entry['attack']=[0]+row['attack']\n                elif self.opt.DATASET=='harm':\n                    if label==0:\n                        #[1,0,0,0,0,0]\n                        entry['attack']=[1,0,0,0,0]\n                    else:\n                        attack=[0,0,0,0,0]\n                        attack_idx=self.attack_list[self.attack_file[img]]+1\n                        attack[attack_idx]=1\n                        entry['attack']=attack\n            entries.append(entry)\n        return entries\n\n    def enc(self,text):\n        return self.tokenizer.encode(text, add_special_tokens=False)\n\n    def prepare_exp(self):\n        ###add sampling\n        support_indices = list(range(len(self.support_examples)))\n        self.example_idx = []\n        for sample_idx in tqdm(range(self.num_sample)):\n            for query_idx in range(len(self.entries)):\n                if self.opt.DEM_SAMP:\n                    #filter dissimilar demonstrations\n                    candidates= [support_idx for support_idx in support_indices\n                                 if support_idx != query_idx or self.mode != \"train\"]\n                    sim_score=[]\n                    count_each_label = {label: 0 for label in range(self.opt.NUM_LABELS)}\n                    context_indices=[]\n                    clip_info_que=self.clip_feature[self.entries[query_idx]['img']]\n\n                    #similarity computation\n                    for support_idx in candidates:\n                        img=self.support_examples[support_idx]['img']\n                        #this cost a lot of computation\n                        #unnormalized: the same scale -- 512 dimension\n                        if self.clip_clean:\n                            img_sim=clip_info_que['clean_img'][img]\n                        else:\n                            img_sim=clip_info_que['img'][img]\n                        text_sim=clip_info_que['text'][img]\n                        total_sim=self.img_rate*img_sim+self.text_rate*text_sim\n                        sim_score.append((support_idx,total_sim))\n                    sim_score.sort(key=lambda x: x[1],reverse=True)\n\n                    #top opt.SIM_RATE entities for each label\n                    num_valid=int(len(sim_score)//self.opt.NUM_LABELS*self.samp_rate)\n                    \"\"\"\n                    if self.opt.DEBUG:\n                        print ('Valid for each class:',num_valid)\n                    \"\"\"\n\n                    for support_idx, score in sim_score:\n                        cur_label=self.support_examples[support_idx]['label']\n                        if count_each_label[cur_label]<num_valid:\n                            count_each_label[cur_label]+=1\n                            context_indices.append(support_idx)\n                else:\n                    #exclude the current example during training\n                    context_indices = [support_idx for support_idx in support_indices\n                                       if support_idx != query_idx or self.mode != \"train\"]\n                #available indexes for supporting examples\n                self.example_idx.append((query_idx, context_indices, sample_idx))\n\n    def select_context(self, context_examples):\n        \"\"\"\n        Select demonstrations from provided examples.\n        \"\"\"\n        num_labels=self.opt.NUM_LABELS\n        max_demo_per_label = 1\n        counts = {k: 0 for k in range(num_labels)}\n        if num_labels == 1:\n            # Regression\n            counts = {'0': 0, '1': 0}\n        selection = []\n        \"\"\"\n        # Sampling strategy from LM-BFF\n        if self.opt.DEBUG:\n            print ('Number of context examples available:',len(context_examples))\n        \"\"\"\n        order = np.random.permutation(len(context_examples))\n        for i in order:\n            label = context_examples[i]['label']\n            if num_labels == 1:\n                # Regression\n                #No implementation currently\n                label = '0' if\\\n                float(label) <= median_mapping[self.args.task_name] else '1'\n            if counts[label] < max_demo_per_label:\n                selection.append(context_examples[i])\n                counts[label] += 1\n            if sum(counts.values()) == len(counts) * max_demo_per_label:\n                break\n\n        assert len(selection) > 0\n        return selection\n\n    def process_prompt(self, examples,\n                       first_sent_limit, other_sent_limit):\n        if self.fine_grind:\n            prompt_arch=' It was targeting '\n        else:\n            prompt_arch=' It was '\n        #currently, first and other limit are the same\n        input_ids = []\n        attention_mask = []\n        mask_pos = None # Position of the mask token\n        concat_sent=\"\"\n        for segment_id, ent in enumerate(examples):\n            #tokens for each example\n            new_tokens=[]\n            if segment_id==0:\n                #implementation for the querying example\n                new_tokens.append(self.special_token_mapping['<s>'])\n                length=first_sent_limit\n                temp=prompt_arch+'<mask>'+' . </s>'\n            else:\n                length=other_sent_limit\n                if self.fine_grind:\n                    if ent['label']==0:\n                        label_word=self.label_mapping_word[0]\n                    else:\n                        attack_types=[i for i, x in enumerate(ent['attack']) if x==1]\n                        #only for meme\n                        if len(attack_types)==0:\n                            attack_idx=random.randint(1,5)\n                        #randomly pick one\n                        #already padding nobody to the head of the list\n                        else:\n                            order=np.random.permutation(len(attack_types))\n                            attack_idx=attack_types[order[0]]\n                        label_word=self.label_mapping_word[attack_idx]\n                else:\n                    label_word=self.label_mapping_word[ent['label']]\n                temp=prompt_arch+label_word+' . </s>'\n            new_tokens+=self.enc(' '+ent['cap'])\n            #truncate the sentence if too long\n            new_tokens=new_tokens[:length]\n            new_tokens+=self.enc(temp)\n            whole_sent=' '+ent['cap']+temp\n            concat_sent+=whole_sent\n\n            #update the prompts\n            input_ids+=new_tokens\n            attention_mask += [1 for i in range(len(new_tokens))]\n        \"\"\"\n        if self.opt.DEBUG and self.opt.DEM_SAMP==False:\n            print (concat_sent)\n        \"\"\"\n        while len(input_ids) < self.total_length:\n            input_ids.append(self.special_token_mapping['<pad>'])\n            attention_mask.append(0)\n        if len(input_ids) > self.total_length:\n            input_ids = input_ids[:self.total_length]\n            attention_mask = attention_mask[:self.total_length]\n        mask_pos = [input_ids.index(self.special_token_mapping['<mask>'])]\n\n        # Make sure that the masked position is inside the max_length\n        assert mask_pos[0] < self.total_length\n        result = {'input_ids': input_ids,\n                  'sent':'<s>'+concat_sent,\n                  'attention_mask': attention_mask,\n                  'mask_pos': mask_pos}\n        return result\n\n\n    def __getitem__(self,index):\n        #query item\n        entry=self.entries[index]\n        #bootstrap_idx --> sample_idx\n        query_idx, context_indices, bootstrap_idx = self.example_idx[index]\n        #one example from each class\n        supports = self.select_context(\n            [self.support_examples[i] for i in context_indices])\n        exps=[]\n        exps.append(entry)\n        exps.extend(supports)\n        prompt_features = self.process_prompt(\n            exps,\n            self.length,\n            self.length\n        )\n\n        vid=entry['img']\n        #label=torch.tensor(self.label_mapping_id[entry['label']])\n        label=torch.tensor(entry['label'])\n        target=torch.from_numpy(np.zeros((self.num_ans),dtype=np.float32))\n        target[label]=1.0\n\n        cap_tokens=torch.Tensor(prompt_features['input_ids'])\n        mask_pos=torch.LongTensor(prompt_features['mask_pos'])\n        mask=torch.Tensor(prompt_features['attention_mask'])\n        batch={\n            'sent':prompt_features['sent'],\n            'mask':mask,\n            'img':vid,\n            'target':target,\n            'cap_tokens':cap_tokens,\n            'mask_pos':mask_pos,\n            'label':label\n        }\n        if self.fine_grind:\n            batch['attack']=torch.Tensor(entry['attack'])\n        #print (batch)\n        return batch\n\n    def __len__(self):\n        return len(self.entries)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:18:51.896396Z","iopub.execute_input":"2024-08-29T20:18:51.896809Z","iopub.status.idle":"2024-08-29T20:18:51.954131Z","shell.execute_reply.started":"2024-08-29T20:18:51.896767Z","shell.execute_reply":"2024-08-29T20:18:51.952876Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import roc_auc_score\nfrom transformers import get_linear_schedule_with_warmup,AdamW","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:18:51.982001Z","iopub.execute_input":"2024-08-29T20:18:51.982377Z","iopub.status.idle":"2024-08-29T20:18:51.987494Z","shell.execute_reply.started":"2024-08-29T20:18:51.982322Z","shell.execute_reply":"2024-08-29T20:18:51.986584Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def eval_multi_model(opt, model, tokenizer):\n    num_queries = opt.NUM_QUERIES\n    labels_record = {}\n    logits_record = {}\n    prob_record = {}\n    correct_predictions = 0\n    total_samples = 0\n\n    for k in range(num_queries):\n        test_set = Multimodal_Data(opt, tokenizer, opt.DATASET, 'test')\n        test_loader = DataLoader(test_set,\n                                 opt.BATCH_SIZE,\n                                 shuffle=False,\n                                 num_workers=1)\n        len_data = len(test_loader.dataset)\n        print('Length of test set:', len_data, 'Query:', k)\n\n        for i, batch in enumerate(test_loader):\n            with torch.no_grad():\n                cap = batch['cap_tokens'].long().cuda()\n                label = batch['label'].float().cuda().view(-1, 1)\n                mask = batch['mask'].cuda()\n                mask_pos = batch['mask_pos'].cuda()\n                logits = model(cap, mask, mask_pos)\n\n                if opt.FINE_GRIND:\n                    logits[:, 1] = torch.sum(logits[:, 1:], dim=1)\n                    logits = logits[:, :2]\n\n                target = batch['target'].cuda()\n                img = batch['img']\n                norm_prob = F.softmax(logits, dim=-1)\n                norm_logits = norm_prob[:, 1].unsqueeze(-1)\n\n                bz = cap.shape[0]\n                for j in range(bz):\n                    cur_img = img[j]\n                    cur_logits = norm_logits[j:j + 1]\n                    cur_prob = norm_prob[j:j + 1]\n                    if k == 0:\n                        cur_label = label[j:j + 1]\n                        labels_record[cur_img] = cur_label\n                        logits_record[cur_img] = cur_logits\n                        prob_record[cur_img] = cur_prob\n                    else:\n                        logits_record[cur_img] += cur_logits\n                        prob_record[cur_img] += cur_prob\n\n                    # Calculate accuracy\n                    pred_label = torch.argmax(norm_prob[j]).item()\n                    true_label = label[j].item()\n                    if pred_label == true_label:\n                        correct_predictions += 1\n                    total_samples += 1\n\n    labels = []\n    logits = []\n    probs = []\n    for name in labels_record.keys():\n        labels.append(labels_record[name])\n        logits.append(logits_record[name] / num_queries)\n        probs.append(prob_record[name] / num_queries)\n\n    logits = torch.cat(logits, dim=0)\n    labels = torch.cat(labels, dim=0)\n    probs = torch.cat(probs, dim=0)\n\n    scores = compute_scaler_score(probs, labels)\n    auc = compute_auc_score(logits, labels)\n\n    accuracy = (correct_predictions / total_samples) * 100.0\n    print(\"accuracy is\",accuracy)\n    return scores * 100.0 / len_data, auc * 100.0 / len_data","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:18:51.989394Z","iopub.execute_input":"2024-08-29T20:18:51.989717Z","iopub.status.idle":"2024-08-29T20:18:52.005916Z","shell.execute_reply.started":"2024-08-29T20:18:51.989685Z","shell.execute_reply":"2024-08-29T20:18:52.005008Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass ModelConfig:\n    DATASET: str\n    FEW_SHOT: bool\n    FINE_GRIND: bool\n    NUM_SHOTS: int\n    MODEL: str\n    UNIMODAL: bool\n    DATA: str\n    CAPTION_PATH: str\n    RESULT: str\n    FEAT_DIM: int\n    CLIP_DIM: int\n    BERT_DIM: int\n    ROBERTA_DIM: int\n    NUM_FOLD: int\n    EMB_DIM: int\n    NUM_LABELS: int\n    POS_WORD: str\n    NEG_WORD: str\n    DEM_SAMP: bool\n    SIM_RATE: float\n    IMG_RATE: float\n    TEXT_RATE: float\n    CLIP_CLEAN: bool\n    MULTI_QUERY: bool\n    NUM_QUERIES: int\n    EMB_DROPOUT: float\n    FC_DROPOUT: float\n    WEIGHT_DECAY: float\n    LR_RATE: float\n    EPS: float\n    BATCH_SIZE: int\n    FIX_LAYERS: int\n    MID_DIM: int\n    NUM_HIDDEN: int\n    LENGTH: int\n    TOTAL_LENGTH: int\n    PREFIX_LENGTH: int\n    NUM_SAMPLE: int\n    NUM_LAYER: int\n    MODEL_NAME: str\n    PRETRAIN_DATA: str\n    IMG_VERSION: str\n    MAPPING_TYPE: str\n    ADD_ENT: bool\n    ADD_DEM: bool\n    DEBUG: bool\n    SAVE: bool\n    SAVE_NUM: int\n    EPOCHS: int\n    SEED: int\n    CUDA_DEVICE: int\n    WARM_UP: int\n    TRANS_LAYER: int\n    NUM_HEAD: int\n    IMAGE_FOLDER:str\n    BASE_MODEL:str\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:18:52.006897Z","iopub.execute_input":"2024-08-29T20:18:52.007194Z","iopub.status.idle":"2024-08-29T20:18:52.023311Z","shell.execute_reply.started":"2024-08-29T20:18:52.007153Z","shell.execute_reply":"2024-08-29T20:18:52.022493Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import RobertaForMaskedLM\nfrom transformers import DistilBertForMaskedLM\nclass RobertaPromptModel(nn.Module):\n    def __init__(self,label_list,base_model):\n        super(RobertaPromptModel, self).__init__()\n        self.label_word_list=label_list\n        self.roberta = RobertaForMaskedLM.from_pretrained(base_model)\n#         self.roberta = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n    def forward(self,tokens,attention_mask,mask_pos,feat=None):\n        batch_size = tokens.size(0)\n        #the position of word for prediction\n        if mask_pos is not None:\n            mask_pos = mask_pos.squeeze()\n\n        out = self.roberta(tokens,\n                           attention_mask)\n        prediction_mask_scores = out.logits[torch.arange(batch_size),\n                                          mask_pos]\n\n        logits = []\n        for label_id in range(len(self.label_word_list)):\n            logits.append(prediction_mask_scores[:,\n                                                 self.label_word_list[label_id]\n                                                ].unsqueeze(-1))\n            #print(prediction_mask_scores[:, self.label_word_list[label_id]].shape)\n        logits = torch.cat(logits, -1)\n        #print(logits.shape)\n        return logits\n\n\ndef build_baseline(opt,label_list):\n    print (label_list)\n    return RobertaPromptModel(label_list,opt.BASE_MODEL)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:18:52.024356Z","iopub.execute_input":"2024-08-29T20:18:52.024615Z","iopub.status.idle":"2024-08-29T20:18:52.038943Z","shell.execute_reply.started":"2024-08-29T20:18:52.024586Z","shell.execute_reply":"2024-08-29T20:18:52.038090Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"opt = ModelConfig(\n    DATASET=\"mem\", FEW_SHOT=False, FINE_GRIND=False, NUM_SHOTS=16, MODEL=\"pbm\", UNIMODAL=False,\n    DATA=\"/kaggle/input\", CAPTION_PATH=\"/kaggle/input\", RESULT=\"/kaggle/working\", FEAT_DIM=2048, CLIP_DIM=512,\n    BERT_DIM=768, ROBERTA_DIM=1024, NUM_FOLD=5, EMB_DIM=300, NUM_LABELS=2, POS_WORD=\"good\",\n    NEG_WORD=\"bad\", DEM_SAMP=False, SIM_RATE=0.5, IMG_RATE=0.5, TEXT_RATE=0.5, CLIP_CLEAN=False,\n    MULTI_QUERY=True, NUM_QUERIES=4, EMB_DROPOUT=0.0, FC_DROPOUT=0.4, WEIGHT_DECAY=0.01, LR_RATE=1.3e-5,\n    EPS=1e-8, BATCH_SIZE=16, FIX_LAYERS=2, MID_DIM=512, NUM_HIDDEN=512, LENGTH=64, TOTAL_LENGTH=256,\n    PREFIX_LENGTH=10, NUM_SAMPLE=1, NUM_LAYER=8, MODEL_NAME=\"roberta-large\", PRETRAIN_DATA=\"conceptual\",\n    IMG_VERSION=\"clean\", MAPPING_TYPE=\"transformer\", ADD_ENT=True, ADD_DEM=True, DEBUG=False, SAVE=False,\n    SAVE_NUM=100, EPOCHS=1, SEED=1111, CUDA_DEVICE=15, WARM_UP=2000, TRANS_LAYER=1, NUM_HEAD=8,IMAGE_FOLDER='/content/drive/MyDrive/meme/data',\n    BASE_MODEL='roberta-base'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:18:52.041174Z","iopub.execute_input":"2024-08-29T20:18:52.041523Z","iopub.status.idle":"2024-08-29T20:18:52.052280Z","shell.execute_reply.started":"2024-08-29T20:18:52.041490Z","shell.execute_reply":"2024-08-29T20:18:52.051406Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import RobertaForMaskedLM\nfrom transformers import RobertaTokenizer\n# Example label list (replace this with your actual label list)\ntokenizer = RobertaTokenizer.from_pretrained(opt.BASE_MODEL)\ntrain_set=Multimodal_Data(opt,tokenizer,opt.DATASET,'train',opt.SEED-1111)\nlabel_list = [train_set.label_mapping_id[i] for i in train_set.label_mapping_word.keys()]\n\n# Build the model\nmodel = build_baseline(opt, label_list)\ncompile\n# Load the state dictionary\nmodel_path = '/kaggle/input/rpm-model/model.pth'\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:18:52.053443Z","iopub.execute_input":"2024-08-29T20:18:52.053756Z","iopub.status.idle":"2024-08-29T20:19:00.685727Z","shell.execute_reply.started":"2024-08-29T20:18:52.053725Z","shell.execute_reply":"2024-08-29T20:19:00.684743Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Adding exntity information? True\nAdding demographic information? True\nUsing target information? False\nMapping for label 0, word good, index 205\nMapping for label 1, word bad, index 1099\nTemplate: *<s>**sent_0*.*_It_was*label_**</s>*\nTemplate list: ['', '<s>', '', 'sent_0', '.', '_It_was', 'label_', '', '</s>', '']\nLength of supporting example: 8500\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:05<00:00,  5.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"The length of the dataset for: train is: 8500\n[205, 1099]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2141497940.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path)\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"RobertaPromptModel(\n  (roberta): RobertaForMaskedLM(\n    (roberta): RobertaModel(\n      (embeddings): RobertaEmbeddings(\n        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): RobertaEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (lm_head): RobertaLMHead(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (decoder): Linear(in_features=768, out_features=50265, bias=True)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming `opt` is already defined and `eval_multi_model` is your evaluation function\nprint(eval_multi_model(opt, model, tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-08-29T20:19:00.686947Z","iopub.execute_input":"2024-08-29T20:19:00.687271Z","iopub.status.idle":"2024-08-29T20:19:41.225371Z","shell.execute_reply.started":"2024-08-29T20:19:00.687237Z","shell.execute_reply":"2024-08-29T20:19:41.224146Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Adding exntity information? True\nAdding demographic information? True\nUsing target information? False\nMapping for label 0, word good, index 205\nMapping for label 1, word bad, index 1099\nTemplate: *<s>**sent_0*.*_It_was*label_**</s>*\nTemplate list: ['', '<s>', '', 'sent_0', '.', '_It_was', 'label_', '', '</s>', '']\nLength of supporting example: 8500\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  3.23it/s]","output_type":"stream"},{"name":"stdout","text":"The length of the dataset for: test is: 500\nLength of test set: 500 Query: 0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Adding exntity information? True\nAdding demographic information? True\nUsing target information? False\nMapping for label 0, word good, index 205\nMapping for label 1, word bad, index 1099\nTemplate: *<s>**sent_0*.*_It_was*label_**</s>*\nTemplate list: ['', '<s>', '', 'sent_0', '.', '_It_was', 'label_', '', '</s>', '']\nLength of supporting example: 8500\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  3.21it/s]","output_type":"stream"},{"name":"stdout","text":"The length of the dataset for: test is: 500\nLength of test set: 500 Query: 1\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Adding exntity information? True\nAdding demographic information? True\nUsing target information? False\nMapping for label 0, word good, index 205\nMapping for label 1, word bad, index 1099\nTemplate: *<s>**sent_0*.*_It_was*label_**</s>*\nTemplate list: ['', '<s>', '', 'sent_0', '.', '_It_was', 'label_', '', '</s>', '']\nLength of supporting example: 8500\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  2.95it/s]","output_type":"stream"},{"name":"stdout","text":"The length of the dataset for: test is: 500\nLength of test set: 500 Query: 2\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Adding exntity information? True\nAdding demographic information? True\nUsing target information? False\nMapping for label 0, word good, index 205\nMapping for label 1, word bad, index 1099\nTemplate: *<s>**sent_0*.*_It_was*label_**</s>*\nTemplate list: ['', '<s>', '', 'sent_0', '.', '_It_was', 'label_', '', '</s>', '']\nLength of supporting example: 8500\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  3.28it/s]","output_type":"stream"},{"name":"stdout","text":"The length of the dataset for: test is: 500\nLength of test set: 500 Query: 3\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"accuracy is 57.9\n(tensor(60.4000, device='cuda:0'), 66.74401113760382)\n","output_type":"stream"}]}]}